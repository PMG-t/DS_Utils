###############################################################################

Understanding society's perception of cities

###############################################################################

1. Overview and goal

Tutte le città hanno caratteristiche che le definiscono: spesso la gente associa
ad una città delle parole che la rappresentano, come la moda di Milano, il lusso
di Dubai, l’emancipazione di Amsterdam. 

Le percezioni possono spaziare su diversi ambiti e possono quindi essere più o 
meno positive. Spesso convergono anche in stereotipi non necessariamente veritieri.

Si vuole quindi analizzare come vengono percepite e dipinte le città dalla società,
ed in particolare al collegamento tra esse, evidenziando cosa hanno in comune.

--------------------------------------------------------------------------------

2. Workflow

- Individuazione del corpus
- Descrizione, analisi e preprocessing del corpus → PoS, y_stop, lemmatization, bigrams
- Costruzione modello e comparazione → cbow / skipgram, test analogie leonardo?
- Step 1: Estrarre per una città delle parole relative alle sue caratteristiche/fama
  - L'idea è dividere le possibili caratteristiche in topic di appartenenza i quali 
    sono identificati da parole chiave
  - Metodo 1 - Supervised:
    - Identificazione topic + parole chiave
    - Algoritmo
    - Risultati
    - Pro / Contro
  - Metodo 2 - Unsupervised:
    - Identificazione topic + parole chiave
    - Algoritmo 
    - Risultati 
    - Pro / Contro
- Step 2: Rappresentazione dei risultati 
  - WordCloud 
    - risultati ottenuti da metodo 1 e 2
  - Grafo → Frequenza Topic, Coesione, Connessione
    - risultati ottenuti da metodo 1 e 2
- Step 3: Interpretazione risultati
  - Utilizzo di weat 
    - Motivazione → disambiguazione forti collegamenti e/o paragoni soggettivi
    - Definizione casi di utilizzo → environment / food / culture / ... ?
    - Metodo 1 - Topic 
      - Algoritmo
    - Metodo 2 - Analogie
      - Algoritmo
    - Run test e confronto 
- Risultati
  - Recap di quello che abbiamo ottenuto, stereotipi
  - Limiti
- Conclusioni / Sviluppi futuri

---------------------------------------------------------------------------------

  ...

---------------------------------------------------------------------------------

Descrizione, analisi e preprocessing del corpus
	
	Il corpus, per ogni sua categoria, oltre a presentare i testi in forma originale,
	fornisce anche una trascrizione standardizzata, per la quale ogni vocabolo è ridotto
	alla sua forma base, associando a questa un tag PoS che ne identifica l'analisi grammaticale.
	Ritenendo che nell'analisi che si vuole condurre i tempi verbali ed i plurali non siano 
	di interesse si preferisce utilizzare questo secondo tipo di formattazione dove i vocaboli 
	sono 'standardizzati', semplificando poi ulteriormente alcuni tag composti, (es: 'tag1_tag2')
	conservando solo il primo ruolo 'tag1'. 

	In secondo luogo viene messo in atto un processo di lemmatizzazione: vocaboli che rappresentano 
	pronomi e congiunzioni oppure simboli per loro natura non rappresentano temi, soggetti, fatti 
	caratteristici di una città, vengono quindi scartati e mantenuti solamente termini che sono 
	nomi (N), aggettivi (J), avverbi (R) o verbi (V).
	Vengono comunque effettuati alcuni test alla cazzo di cane per vedere se ha senso:
	[Screen che avevamo fatto]

	Infine allo scopo di rappresentare nel modello termini composti, i quali possono essere 
	sia vocaboli generali, che anche alcune città importanti ('new york', 'rio de janeiro'),
	viene attuata una ricerca dei bigrammi. 
	[Cosa fa ? vai mc !]
	
	Così facendo 'new york' viene trasformata in un singolo vocabolo 'new_york', mentre
	'rio de janeiro' viene trasformato in 'rio_janeiro' (in questo caso il 'de' viene rimosso
	precedentemente dalla lemmatizzazione, il metodo dei bigrammi riesce quindi successivamente
	ad associare il termine 'rio' al termne 'janeiro')

---------------------------------------------------------------------------------

Costruzione modello e comparazione

	Il passo immediatamente successivo è la realizzazione di un modello di 
	semantica distribiuita attraverso i dati preprocessati.
	Per questo scopo, sono stati realizzati inizialmente diversi modelli tramite
	la tecnica Word2Vec, per la quale sono stati presi in considerazione sia 
	CBOW che SkipGram. 
	
	Dato lo scopo della nostra analisi si è pensato inizialmente che SkipGram
	fosse la modalità migliore, in quanto partendo da un singolo termine (una città)
	vogliamo ottenere diversi termini con annessa una probabilità di associazione 
	a questa.

	Ad ogni modo vengono comunque realizzati entrambi i modelli, quindi viene 
	effettuato un test di verifica sulla precisione delle analogie, il quale ha 
	poi decretato SkipGram come modello effettivamente migliore.
	[Leonardo parlaci dei test]

---------------------------------------------------------------------------------

Step 1: Estrarre per una città delle parole relative alle sue caratteristiche

	Come primo obiettivo ci siamo preposti quello di estrarre termini che 
	rappresentassero le città dal punto di vista dell'immaginario collettivo.

	A questo fine si è scelto preventivamente di suddividere i diversi ambiti che 
	un'intera idea di città contiene. Abbiamo quindi generato una lista di topic, 
	per i quali, ad uno ad uno e per ogni città, si estrarranno termini legati si
	al topic stesso che alla città in quel momento sotto indagine.

	Lo scopo è quindi quello di ottenere per ogni città, una lista di termini che la
	rappresentano in diversi ambiti (che già conosciamo), così da poterne osservare
	le numerosità, similarità e coesione ed eventualmente distinguere poi le diverse 
	accezioni che vengono attribute. 

---------------------------------------------------------------------------------

	...

---------------------------------------------------------------------------------

Metodo 2 - Unsupervised (cristo jupyter)

	1. Idea: 
		Data una città (es: san paolo brazil) e un topic generico (es: crime o pollution),
		si può possono formulare delle analogie per cercare di ottenere i riferimenti della
		specifica città in quello specifico ambito.

		es 1: 'city : crime = sao_paulo : x' 
					→ 'violent_crime', 'homicide', 'perpetrator', 'carjacking', 'drug_trafficking',
					 'drug_related', 'tribunal_hague', 'kidnapping', 'offender', 'commit_crime', 'robbery'
		
		es 1: 'city : pollution = sao_paulo : x' 
					→ 'air_pollution', 'carbon_emission', 'emission', 'eutrophication', 'ozone_depletion', 
					'ghg_emission', 'nitrogen_oxide', 'deforestation', 'sulfur_dioxide', 'agrochemical'

	2. Impostazione: 
		definizione dei topic, e per ognuno di questi due o tre parole chiave che lo rappresentano:
		{ 'crime': ['crime'],
			'education': ['education','school'],
			'food': ['food','recipe', 'alcohol'],
			'sociality': ['politic','economy'],
			'tourism': ['tourism','vacation'],
			'environment': ['environment','pollution'],
			'history': ['history','folklore'],
			'culture': ['music','dance','art'],
			'geo': ['vegetation','flora_fauna']	}

	3. Algoritmo:
		- Raccolta parole:
			Per ogni città, per ogni parola chiave di ciascun topic si ricavano tramite analogia 
			le parole che dovrebbero rappresentare quella caratteristica in quella città.
			
			→ s.wanal('city : ' + keyword + ' = ' + city + ':x', thresh=0.3, topn=40)
				- viene impostato un limite inferiore di similarità a 0.3, e ricavate le prime 40 parole
				- causa merda vengono scartati ri siultati che non hanno diretta similarità con 
					'keyword' e 'city' maggiore di 0.2

		- Clustering parole:
			L'insieme delle parole raccolte è numeroso e queste possono rigurardare diversi
			aspetti dell'ambito indagato. Al fine successivo di osservare sia la coesione
			interna di una città che il grado di similarità tra due di queste, può risultare
			utile aggregare preventivamente la lista di termini ottenuti.

			Viene applicato un clustering DBSCAN (Density-Based Spatial Clustering of Applications with Noise)
			ponendo come input la matrice delle distanze tra le parole raccolte. [Fotina]
			La scelta di DBSCAN sta nella di N di Noise, il metodo è di clustering parziale, parole quindi 
			che non hanno nessi significativi con ognuna delle altre vengono scartate.

			Impostazioni significative del clustering:
			- eps: media(distance matrix) - std(distance matrix)
			- min_samples: 2 

			[Fotina set_prima → set_clusterizzato_dopo]

	4. Risultati:
		- analogia notevole 1 - napoli ⟷ food
		- analogia notevole 2 - kinshasa ⟷ crime
		- analogia notevole 3 - athens ⟷ history
		- analogia negativa 4 - culture ⟷ rome → parole vere ma generiche / poco mirate / comuni

	5. Pro e contro:
		Il metodo permette indubbiamente di individuare elementi o concetti più specifici e 
		caratteristici della città. Il metodo supervisionato non potrebbe raggiungere lo stesso 
		livello di particolarità a meno che le liste preimpostate non vengano ingigantite
		con le specifiche modalità che un topic ppuò assumere.
		A discapito di questo però vi è però il problema di raccogliere termini generali,
		non specifici della città anzi comuni a diverse di queste (environment → climate_change),
		oppure ancora termini non del tutto attinenti all'ambito (es: food → autore_libro_cucina)

---------------------------------------------------------------------------------

	...

---------------------------------------------------------------------------------

Grafo (cristo jupyter)

	1. Idea:
		L'idea è rappresentare i dati ottenuti attraverso un grafo i cui nodi rappresentano
		le città e gli archi dei collegamenti più o meno forti tra il contenuto dei topic 
		di queste.
		La visualizzazione a grafo è ideale perchè permette di poter definire diversi attributi
		sulle entità rappresentate. Le città si possono infatti raffigurare diversamente
		per predominanza di un topic piuttosto che un altro, per coesione interna di questi e 
		per diverso grado di collegamento il quale può essere più o meno forte. 
		Viene realizzato un grafo per ogni singolo topic indagato.

	2. Dimensione del nodo: 
		I nodi delle città, sono rappresentati da un pallino la cui dimensione è proporzionale
		alla frequenza relativa delle parole legate a quel topic rispetto a tutte le parole 
		raccolte in tutti i topic per quella città.

	3. Colore del nodo:
		Il colore di un nodo città può essere più o meno intenso a seconda della coesione dei 
		termini raccolti. Dati i vocaboli raccolti sotto un topic, e la loro divisione in 
		cluster, lo score di coesione è dato dalla sommatoria, per ogni cluster, del prodotto
		di coesione e frequenza relativa, così da favorire quei punti sia con elevata coesione
		sia con una buona predominanza.

	4. Grado di collegamento:
		Per quanto riguarda il collegamento tra due città in un certo ambito sono stati
		testate diversi metodi. Il metodo scelto per l'applicazione si basa sul calcolo della
		distanza tra i due vettori medi delle due città messe a confronto. Il vettore medio 
		di una città viene calcolato ottenendo per ogni cluster il vettore medio delle parole 
		che lo compongono e infine, mediando questi valori medi per ottenere un singolo vettore 
		rappresentativo. 
		Questo metodo si rivela piuttosto adeguato, esso permette di associare città in base alla 
		similarità dei termini e non esclusivamente alla loro uguaglianza; inoltre, a visualizzazione
		realizzata, permette di individuare cricche di città fortemente connesse tra loro e
		maggiormente isolate rispetto alle altre (es: food → beijing, seoul, singapore)

	5. Risultati ottenuti per metodo 1 (supervised) e metodo 2 (unsupervised)
		I risultati ottenuti per i due diversi metodi di estrazione delle parole sono simili
		e coincidono a grandi linee, le differenze sono figlie dei pro e contro di questi. 	
		.... to be continued

---------------------------------------------------------------------------------

	...

---------------------------------------------------------------------------------

Step 3: Interpretazione risultati

	I risultati ottenuti, rappresentati sia tramite word cloud che grafo permettono
	già una buona esplorazione delle aree tematiche delle città e dei collegamenti
	che si creano tra queste.
	Tuttavia è possibile notare alcune situazioni la cui interpretazione risulta 
	dubbia, poco solida o insapettata.

	Per tentare di arginare questa cosa abbiamo scelto di utilizzare WEAT ...
	... to be continued

---------------------------------------------------------------------------------

	...

---------------------------------------------------------------------------------

Metodo 2 - Analogie (weat test jupyter)

	1. Idea: 
		Dal momento che si vuole disambiguare l'accezione dei termini di un particolare
		ambito in diverse città l'idea ricade ancora sullo sfruttare le analogie per 
		ottenere set di termini relativi si allo stesso topic, ma da due punti di vista 
		opposti.

	2. Algoritmo:
		- Definizione dei due set di città messi a confronto: 
			es: city_set1 = ['oslo', 'copenhagen', 'stockholm', 'brussels'] vs.
					city_set2 = ['brasilia', 'sao_paulo', 'rio_janeiro']

		- Definizione dei termini di disambiguazione:
		  - Topic in questione (es: 'environment')
			- Soggetto all'interno del topic (es: 'pollution')
			- Punto di vista positivo (es: 'protection')
			- Punto di vista cause (es: 'cause')

		- Definizione delle analogie:
			- analogia positiva: Topic : Subject = PV Positivo : x (es: environment : protection = pollution : x)
				→ ['protect', 'enforcement', 'air_pollution', 'safeguard', 'environmental_protection',
					'prevention', 'abatement', 'anti_pollution', 'nonpoint_source', 'naaqs']

			- analogia negativa: Topic : Subject = PV Negativo : x (es: environment : cause = pollution : x)
				→ ['air_pollution', 'nonpoint_source', 'culprit', 'pollutant', 'sulphur_dioxide', 
					'particulate', 'traffic_related', 'sulfur_dioxide', 'smog', 'headache_dizziness']

		- Chiamata a WEAT:
			w.weat_m1(city_set1, city_set2, 
								Topic='environment', Subject='pollution', Positive='protection', Negative='cause',
								p_value=True, verbose=True)
			↓
			WEAT(	X = ['oslo', 'copenhagen', 'stockholm', 'brussels'], 
						Y = ['brasilia', 'sao_paulo', 'rio_janeiro'],
						A = ['protect', 'enforcement', 'air_pollution', 'safeguard', 'environmental_protection',
								'prevention', 'abatement', 'anti_pollution', 'nonpoint_source', 'naaqs'],
						B = ['air_pollution', 'nonpoint_source', 'culprit', 'pollutant', 'sulphur_dioxide', 
								'particulate', 'traffic_related', 'sulfur_dioxide', 'smog', 'headache_dizziness'])

---------------------------------------------------------------------------------

	...

---------------------------------------------------------------------------------